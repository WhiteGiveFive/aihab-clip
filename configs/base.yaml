# ------ Paths and dataset ------
root_path: './'            # base path for feature caches and outputs
dataset: 'cs'              # custom CS dataset identifier
output_dir: './results'

# ------ Projector training switches ------
finetune:
  enabled: True               # set True to run ProLIP training/eval
  tune_text: True            # when True (OpenCLIP), fine-tune text tower and recompute text_weights from prompt_tokens
  unlocked_groups: 11         # number of vision tower groups to unfreeze for OpenCLIP fine-tuning
  unlocked_layers: 1          # number of text tower layers to unfreeze for OpenCLIP fine-tuning (used when tune_text=True)
  val_interval: 10
  eval_l2: False              # report L2 metrics in validation/test when True
  l2_eval_mode: 'argmax'      # L2 evaluation mode: 'argmax' (map L3 argmax) or 'logits' (aggregate L3 logits)
  eval_only: False             # skip training, only evaluate projector
  checkpoint: null             # optional path to saved projector weights
  output_dir: './results'      # where to write ProLIP outputs/checkpoints, not used at this moment.
  require_cached_features: True  # ensure feature cache exists before training
wandb_project: 'aihab-clip'

# ------ CLIP / Model settings ------
clip_backend: 'openclip'         # openai | openclip
backbone: 'ViT-B/32'           # RN50 | RN101 | ViT-B/16 | ViT-B/32 (OpenAI CLIP names)
open_clip_model: 'hf-hub:timm/ViT-SO400M-16-SigLIP2-384'    # OpenCLIP model name (used when clip_backend=openclip)
open_clip_pretrained: null # OpenCLIP pretrained tag (e.g., laion2b_s34b_b79k)
open_clip_cache_dir: null      # optional path override for OpenCLIP checkpoints
use_model_preprocess: True     # when True and clip_backend=openclip, use the model's native preprocess transforms
use_hierarchical_prompts: True # toggle hierarchical (L2+L3) prompts vs flat L3-only prompts
use_descriptive_prompts: True   # toggle descriptive templates for Grassland L3 classes
SUBSAMPLE_CLASSES: 'all'

# ------ ProLIP training (projector) ------
method: 'ProLIP'
train_epoch: 5           # projector training epochs (few-shot can use 300)
lr_v: 0.00005              # projector learning rate
lambda_v: 0.1              # fallback regularization strength
lambda_funct_1_N: False     # if True, lambda = 1/N (shots), make it False when shots = 0!
lambda_funct_1_N2: False   # alternative lambda = 1/N^2, make it False when shots = 0!
search_lr: True           # disable grid-search by default
feat_batch_size: 64         # 0 = full-batch (few-shot); >0 enables chunking for full-data
save_checkpoints: False

# ------ Feature caching ------
save_features: False       # set True to precompute and cache pre-projection features
aug_views: 3               # number of augmentation views to cache (use 300 for few-shot)

# ------ Training mode (loader control) ------
shots: 0                   # 0 (or <1) -> full-data; N>0 -> few-shot per class
seed: 1
subset_l2_names: []  # e.g. ['Grassland', 'Wetland']; empty => use all L3 classes
