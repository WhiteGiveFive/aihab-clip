{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cached CLIP pre-projection features\n",
    "Quick notebook to reproduce the cached-feature loading logic from `methods/ProLIP.py` (lines 103–114)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/hshi/Documents/researchproject/aihab/repo/aihab-clip\n",
      "CWD: /home/hshi/Documents/researchproject/aihab/repo/aihab-clip/notebooks\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Project root (repo), assuming this notebook lives in `notebooks/`\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"CWD: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting cached features at: /home/hshi/Documents/researchproject/aihab/repo/aihab-clip/features_ViTB32_cs/0_shot/seed1\n",
      "Contents:\n",
      " - f0.pth\n",
      " - label.pth\n"
     ]
    }
   ],
   "source": [
    "# Configuration — adjust to match the cached features you have\n",
    "# backbone: one of [\"RN50\", \"RN101\", \"ViT-B/16\", \"ViT-B/32\"]\n",
    "cfg = {\n",
    "    \"backbone\": \"ViT-B/32\",\n",
    "    \"dataset\": \"cs\",    # dataset id used in feature folder name\n",
    "    \"shots\": 0,         # number of shots used when caching\n",
    "    \"seed\": 1,           # corresponds to `task` in ProLIP\n",
    "    \"aug_views\": 1,      # number of augmented views saved\n",
    "    \"root_path\": str(PROJECT_ROOT),\n",
    "    \"train_epoch\" : 1,    # number of projector-training epochs (cfg['train_epoch'])\n",
    "    \"lr_v\" : 1e-5,        # learning rate for Adam on the projector (cfg['lr_v'])\n",
    "    \"lambda_v\" : 1.0,     # weight on the MSE regularizer vs. original projection (cfg['lambda_v'])\n",
    "    \"aug_views\" : 1,      # how many cached augmentation views you cycle through per epoch (cfg['aug_views'])\n",
    "    \"feat_batch_size\" : 0   # 0 => use the entire cached tensor each step (few-shot default)\n",
    "                           # >0 => chunk the cached features into mini-batches of this size to save memory (full-data mode)\n",
    "}\n",
    "\n",
    "def _canonical_backbone_name(backbone: str) -> str:\n",
    "    if backbone == \"ViT-B/16\":\n",
    "        return \"ViTB16\"\n",
    "    if backbone == \"ViT-B/32\":\n",
    "        return \"ViTB32\"\n",
    "    return backbone\n",
    "\n",
    "backbone_name = _canonical_backbone_name(cfg[\"backbone\"])\n",
    "feature_dir = Path(cfg[\"root_path\"]) / f\"features_{backbone_name}_{cfg['dataset']}\" / f\"{cfg['shots']}_shot\" / f\"seed{cfg['seed']}\"\n",
    "print(f\"Expecting cached features at: {feature_dir}\")\n",
    "\n",
    "if not feature_dir.exists():\n",
    "    raise FileNotFoundError(f\"Feature directory not found: {feature_dir}\\nCheck cfg settings above.\")\n",
    "\n",
    "# Show what's inside for quick sanity check\n",
    "print(\"Contents:\")\n",
    "for p in sorted(feature_dir.glob(\"*\")):\n",
    "    print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.pth loaded: torch.Size([4200]) torch.int64\n",
      "Filtered labels shape: torch.Size([4200])\n",
      "Loaded f0.pth -> torch.Size([4200, 768]), dtype=torch.float16\n",
      "Total views loaded: 1\n"
     ]
    }
   ],
   "source": [
    "# Load labels and filter to in-range classes (mirrors ProLIP lines 103-114)\n",
    "label_path = feature_dir / \"label.pth\"\n",
    "train_labels = torch.load(label_path, weights_only=True)\n",
    "print(\"label.pth loaded:\", train_labels.shape, train_labels.dtype)\n",
    "\n",
    "# If you know the exact number of classes, set m directly; here we take a safe upper bound\n",
    "m = train_labels.max().item() + 1\n",
    "indices = torch.where(train_labels < m)[0]\n",
    "train_labels = train_labels[indices]\n",
    "print(\"Filtered labels shape:\", train_labels.shape)\n",
    "\n",
    "train_x_before_list = []\n",
    "for num in range(cfg['aug_views']):\n",
    "    fpath = feature_dir / f\"f{num}.pth\"\n",
    "    feats = torch.load(fpath, weights_only=True)\n",
    "    feats = feats[indices]\n",
    "    train_x_before_list.append(feats)\n",
    "    print(f\"Loaded f{num}.pth -> {feats.shape}, dtype={feats.dtype}\")\n",
    "\n",
    "print(f\"Total views loaded: {len(train_x_before_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f91b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample feature vector length: torch.Size([4200, 768])\n",
      "Sample feature vector length: 768\n",
      "First 5 values: tensor([ 1.4717,  1.2275, -0.1714,  0.2610,  1.8027], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Example: inspect a sample vector from the first view\n",
    "if train_x_before_list:\n",
    "    print(\"Sample feature vector length:\", train_x_before_list[0].shape)\n",
    "    sample = train_x_before_list[0][0]\n",
    "    print(\"Sample feature vector length:\", sample.numel())\n",
    "    print(\"First 5 values:\", sample[:5])\n",
    "else:\n",
    "    print(\"No views loaded — check cfg['aug_views'] and that f0.pth exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77694068",
   "metadata": {},
   "source": [
    "# Create a projector, text weights, and config for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2019621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, copy\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from methods.ProLIP import VisProjViT  # imports the class\n",
    "from utils import clip_classifier\n",
    "from data.templates import CS_TEMPLATES, CS_CLASSNAMES  # swap if using another dataset\n",
    "from methods.utils import cls_acc, compute_image_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37313262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_weights shape: torch.Size([512, 20])\n"
     ]
    }
   ],
   "source": [
    "import clip  # this repo’s bundled CLIP package\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "state_dict, clip_model, preprocess = clip.load(cfg[\"backbone\"], device=device)\n",
    "texts, text_weights_before, text_weights = clip_classifier(CS_CLASSNAMES, CS_TEMPLATES, clip_model)\n",
    "print(\"text_weights shape:\", text_weights.shape) \n",
    "\n",
    "\n",
    "vit_proj = state_dict[\"visual.proj\"]\n",
    "proj = VisProjViT(vit_proj)\n",
    "\n",
    "# For testing purposes, assumes you already built train_x_before_list as in your notebook\n",
    "# batch_x = train_x_before_list[0].to(proj.vit_proj.device)\n",
    "# image_features = proj(batch_x)  # shape: [batch, embed_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf26d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ProLIP hyperparameters ===\n",
    "train_epoch = 200          # number of projector-training epochs (cfg['train_epoch'])\n",
    "lr_v = cfg[\"lr_v\"]                # learning rate for Adam on the projector (cfg['lr_v'])\n",
    "lambda_v = cfg[\"lambda_v\"]             # weight on the MSE regularizer vs. original projection (cfg['lambda_v'])\n",
    "aug_views = cfg[\"aug_views\"]              # how many cached augmentation views you cycle through per epoch (cfg['aug_views'])\n",
    "feat_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e029b5",
   "metadata": {},
   "source": [
    "# Training the projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a569aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Projector training ==\n",
      "Epoch 1/200 | Acc 0.2612 | Loss_ce 2.5625 | Loss_mse 0.0000\n",
      "Epoch 2/200 | Acc 0.2888 | Loss_ce 2.3984 | Loss_mse 0.0100\n",
      "Epoch 3/200 | Acc 0.3157 | Loss_ce 2.2637 | Loss_mse 0.0334\n",
      "Epoch 4/200 | Acc 0.3376 | Loss_ce 2.1504 | Loss_mse 0.0635\n",
      "Epoch 5/200 | Acc 0.3586 | Loss_ce 2.0547 | Loss_mse 0.0958\n",
      "Epoch 6/200 | Acc 0.3810 | Loss_ce 1.9727 | Loss_mse 0.1266\n",
      "Epoch 7/200 | Acc 0.3945 | Loss_ce 1.9043 | Loss_mse 0.1537\n",
      "Epoch 8/200 | Acc 0.4124 | Loss_ce 1.8467 | Loss_mse 0.1759\n",
      "Epoch 9/200 | Acc 0.4260 | Loss_ce 1.7969 | Loss_mse 0.1927\n",
      "Epoch 10/200 | Acc 0.4355 | Loss_ce 1.7549 | Loss_mse 0.2041\n",
      "Epoch 11/200 | Acc 0.4424 | Loss_ce 1.7178 | Loss_mse 0.2103\n",
      "Epoch 12/200 | Acc 0.4500 | Loss_ce 1.6865 | Loss_mse 0.2119\n",
      "Epoch 13/200 | Acc 0.4579 | Loss_ce 1.6602 | Loss_mse 0.2124\n",
      "Epoch 14/200 | Acc 0.4640 | Loss_ce 1.6377 | Loss_mse 0.2073\n",
      "Epoch 15/200 | Acc 0.4690 | Loss_ce 1.6182 | Loss_mse 0.1997\n",
      "Epoch 16/200 | Acc 0.4724 | Loss_ce 1.6025 | Loss_mse 0.1908\n",
      "Epoch 17/200 | Acc 0.4764 | Loss_ce 1.5898 | Loss_mse 0.1810\n",
      "Epoch 18/200 | Acc 0.4783 | Loss_ce 1.5791 | Loss_mse 0.1710\n",
      "Epoch 19/200 | Acc 0.4783 | Loss_ce 1.5693 | Loss_mse 0.1614\n",
      "Epoch 20/200 | Acc 0.4812 | Loss_ce 1.5615 | Loss_mse 0.1525\n",
      "Epoch 21/200 | Acc 0.4824 | Loss_ce 1.5557 | Loss_mse 0.1460\n",
      "Epoch 22/200 | Acc 0.4840 | Loss_ce 1.5498 | Loss_mse 0.1407\n",
      "Epoch 23/200 | Acc 0.4852 | Loss_ce 1.5439 | Loss_mse 0.1351\n",
      "Epoch 24/200 | Acc 0.4871 | Loss_ce 1.5391 | Loss_mse 0.1343\n",
      "Epoch 25/200 | Acc 0.4890 | Loss_ce 1.5352 | Loss_mse 0.1296\n",
      "Epoch 26/200 | Acc 0.4902 | Loss_ce 1.5303 | Loss_mse 0.1259\n",
      "Epoch 27/200 | Acc 0.4907 | Loss_ce 1.5254 | Loss_mse 0.1232\n",
      "Epoch 28/200 | Acc 0.4931 | Loss_ce 1.5205 | Loss_mse 0.1208\n",
      "Epoch 29/200 | Acc 0.4924 | Loss_ce 1.5166 | Loss_mse 0.1190\n",
      "Epoch 30/200 | Acc 0.4924 | Loss_ce 1.5117 | Loss_mse 0.1177\n",
      "Epoch 31/200 | Acc 0.4926 | Loss_ce 1.5068 | Loss_mse 0.1168\n",
      "Epoch 32/200 | Acc 0.4940 | Loss_ce 1.5029 | Loss_mse 0.1162\n",
      "Epoch 33/200 | Acc 0.4931 | Loss_ce 1.4980 | Loss_mse 0.1158\n",
      "Epoch 34/200 | Acc 0.4936 | Loss_ce 1.4932 | Loss_mse 0.1157\n",
      "Epoch 35/200 | Acc 0.4943 | Loss_ce 1.4893 | Loss_mse 0.1158\n",
      "Epoch 36/200 | Acc 0.4962 | Loss_ce 1.4854 | Loss_mse 0.1160\n",
      "Epoch 37/200 | Acc 0.4993 | Loss_ce 1.4805 | Loss_mse 0.1163\n",
      "Epoch 38/200 | Acc 0.4998 | Loss_ce 1.4766 | Loss_mse 0.1168\n",
      "Epoch 39/200 | Acc 0.5014 | Loss_ce 1.4727 | Loss_mse 0.1172\n",
      "Epoch 40/200 | Acc 0.5026 | Loss_ce 1.4688 | Loss_mse 0.1177\n",
      "Epoch 41/200 | Acc 0.5038 | Loss_ce 1.4648 | Loss_mse 0.1200\n",
      "Epoch 42/200 | Acc 0.5050 | Loss_ce 1.4619 | Loss_mse 0.1204\n",
      "Epoch 43/200 | Acc 0.5052 | Loss_ce 1.4580 | Loss_mse 0.1208\n",
      "Epoch 44/200 | Acc 0.5062 | Loss_ce 1.4551 | Loss_mse 0.1212\n",
      "Epoch 45/200 | Acc 0.5069 | Loss_ce 1.4521 | Loss_mse 0.1216\n",
      "Epoch 46/200 | Acc 0.5081 | Loss_ce 1.4492 | Loss_mse 0.1220\n",
      "Epoch 47/200 | Acc 0.5086 | Loss_ce 1.4463 | Loss_mse 0.1223\n",
      "Epoch 48/200 | Acc 0.5093 | Loss_ce 1.4434 | Loss_mse 0.1226\n",
      "Epoch 49/200 | Acc 0.5112 | Loss_ce 1.4414 | Loss_mse 0.1227\n",
      "Epoch 50/200 | Acc 0.5124 | Loss_ce 1.4385 | Loss_mse 0.1229\n",
      "Epoch 51/200 | Acc 0.5112 | Loss_ce 1.4365 | Loss_mse 0.1229\n",
      "Epoch 52/200 | Acc 0.5119 | Loss_ce 1.4336 | Loss_mse 0.1238\n",
      "Epoch 53/200 | Acc 0.5117 | Loss_ce 1.4316 | Loss_mse 0.1237\n",
      "Epoch 54/200 | Acc 0.5129 | Loss_ce 1.4297 | Loss_mse 0.1235\n",
      "Epoch 55/200 | Acc 0.5126 | Loss_ce 1.4277 | Loss_mse 0.1233\n",
      "Epoch 56/200 | Acc 0.5119 | Loss_ce 1.4258 | Loss_mse 0.1230\n",
      "Epoch 57/200 | Acc 0.5129 | Loss_ce 1.4248 | Loss_mse 0.1226\n",
      "Epoch 58/200 | Acc 0.5136 | Loss_ce 1.4229 | Loss_mse 0.1221\n",
      "Epoch 59/200 | Acc 0.5148 | Loss_ce 1.4219 | Loss_mse 0.1216\n",
      "Epoch 60/200 | Acc 0.5145 | Loss_ce 1.4199 | Loss_mse 0.1212\n",
      "Epoch 61/200 | Acc 0.5155 | Loss_ce 1.4189 | Loss_mse 0.1213\n",
      "Epoch 62/200 | Acc 0.5155 | Loss_ce 1.4180 | Loss_mse 0.1207\n",
      "Epoch 63/200 | Acc 0.5167 | Loss_ce 1.4160 | Loss_mse 0.1201\n",
      "Epoch 64/200 | Acc 0.5174 | Loss_ce 1.4150 | Loss_mse 0.1196\n",
      "Epoch 65/200 | Acc 0.5167 | Loss_ce 1.4141 | Loss_mse 0.1190\n",
      "Epoch 66/200 | Acc 0.5179 | Loss_ce 1.4131 | Loss_mse 0.1185\n",
      "Epoch 67/200 | Acc 0.5176 | Loss_ce 1.4121 | Loss_mse 0.1179\n",
      "Epoch 68/200 | Acc 0.5181 | Loss_ce 1.4111 | Loss_mse 0.1174\n",
      "Epoch 69/200 | Acc 0.5183 | Loss_ce 1.4102 | Loss_mse 0.1174\n",
      "Epoch 70/200 | Acc 0.5188 | Loss_ce 1.4092 | Loss_mse 0.1170\n",
      "Epoch 71/200 | Acc 0.5195 | Loss_ce 1.4082 | Loss_mse 0.1166\n",
      "Epoch 72/200 | Acc 0.5193 | Loss_ce 1.4072 | Loss_mse 0.1161\n",
      "Epoch 73/200 | Acc 0.5200 | Loss_ce 1.4062 | Loss_mse 0.1158\n",
      "Epoch 74/200 | Acc 0.5202 | Loss_ce 1.4053 | Loss_mse 0.1155\n",
      "Epoch 75/200 | Acc 0.5198 | Loss_ce 1.4043 | Loss_mse 0.1152\n",
      "Epoch 76/200 | Acc 0.5200 | Loss_ce 1.4033 | Loss_mse 0.1149\n",
      "Epoch 77/200 | Acc 0.5207 | Loss_ce 1.4023 | Loss_mse 0.1150\n",
      "Epoch 78/200 | Acc 0.5217 | Loss_ce 1.4014 | Loss_mse 0.1148\n",
      "Epoch 79/200 | Acc 0.5212 | Loss_ce 1.4004 | Loss_mse 0.1146\n",
      "Epoch 80/200 | Acc 0.5212 | Loss_ce 1.4004 | Loss_mse 0.1144\n",
      "Epoch 81/200 | Acc 0.5214 | Loss_ce 1.3994 | Loss_mse 0.1143\n",
      "Epoch 82/200 | Acc 0.5217 | Loss_ce 1.3984 | Loss_mse 0.1142\n",
      "Epoch 83/200 | Acc 0.5224 | Loss_ce 1.3975 | Loss_mse 0.1144\n",
      "Epoch 84/200 | Acc 0.5229 | Loss_ce 1.3965 | Loss_mse 0.1143\n",
      "Epoch 85/200 | Acc 0.5233 | Loss_ce 1.3955 | Loss_mse 0.1142\n",
      "Epoch 86/200 | Acc 0.5236 | Loss_ce 1.3955 | Loss_mse 0.1141\n",
      "Epoch 87/200 | Acc 0.5238 | Loss_ce 1.3945 | Loss_mse 0.1141\n",
      "Epoch 88/200 | Acc 0.5245 | Loss_ce 1.3936 | Loss_mse 0.1141\n",
      "Epoch 89/200 | Acc 0.5240 | Loss_ce 1.3926 | Loss_mse 0.1142\n",
      "Epoch 90/200 | Acc 0.5250 | Loss_ce 1.3916 | Loss_mse 0.1141\n",
      "Epoch 91/200 | Acc 0.5252 | Loss_ce 1.3916 | Loss_mse 0.1141\n",
      "Epoch 92/200 | Acc 0.5252 | Loss_ce 1.3906 | Loss_mse 0.1141\n",
      "Epoch 93/200 | Acc 0.5252 | Loss_ce 1.3896 | Loss_mse 0.1141\n",
      "Epoch 94/200 | Acc 0.5252 | Loss_ce 1.3896 | Loss_mse 0.1141\n",
      "Epoch 95/200 | Acc 0.5255 | Loss_ce 1.3887 | Loss_mse 0.1143\n",
      "Epoch 96/200 | Acc 0.5257 | Loss_ce 1.3877 | Loss_mse 0.1143\n",
      "Epoch 97/200 | Acc 0.5255 | Loss_ce 1.3877 | Loss_mse 0.1143\n",
      "Epoch 98/200 | Acc 0.5260 | Loss_ce 1.3867 | Loss_mse 0.1144\n",
      "Epoch 99/200 | Acc 0.5260 | Loss_ce 1.3857 | Loss_mse 0.1144\n",
      "Epoch 100/200 | Acc 0.5260 | Loss_ce 1.3857 | Loss_mse 0.1145\n",
      "Epoch 101/200 | Acc 0.5264 | Loss_ce 1.3848 | Loss_mse 0.1145\n",
      "Epoch 102/200 | Acc 0.5257 | Loss_ce 1.3848 | Loss_mse 0.1146\n",
      "Epoch 103/200 | Acc 0.5262 | Loss_ce 1.3838 | Loss_mse 0.1146\n",
      "Epoch 104/200 | Acc 0.5262 | Loss_ce 1.3838 | Loss_mse 0.1146\n",
      "Epoch 105/200 | Acc 0.5262 | Loss_ce 1.3828 | Loss_mse 0.1147\n",
      "Epoch 106/200 | Acc 0.5262 | Loss_ce 1.3828 | Loss_mse 0.1147\n",
      "Epoch 107/200 | Acc 0.5267 | Loss_ce 1.3818 | Loss_mse 0.1148\n",
      "Epoch 108/200 | Acc 0.5269 | Loss_ce 1.3818 | Loss_mse 0.1148\n",
      "Epoch 109/200 | Acc 0.5271 | Loss_ce 1.3809 | Loss_mse 0.1149\n",
      "Epoch 110/200 | Acc 0.5279 | Loss_ce 1.3809 | Loss_mse 0.1150\n",
      "Epoch 111/200 | Acc 0.5276 | Loss_ce 1.3799 | Loss_mse 0.1151\n",
      "Epoch 112/200 | Acc 0.5281 | Loss_ce 1.3799 | Loss_mse 0.1151\n",
      "Epoch 113/200 | Acc 0.5283 | Loss_ce 1.3789 | Loss_mse 0.1152\n",
      "Epoch 114/200 | Acc 0.5283 | Loss_ce 1.3789 | Loss_mse 0.1152\n",
      "Epoch 115/200 | Acc 0.5283 | Loss_ce 1.3789 | Loss_mse 0.1153\n",
      "Epoch 116/200 | Acc 0.5288 | Loss_ce 1.3779 | Loss_mse 0.1154\n",
      "Epoch 117/200 | Acc 0.5286 | Loss_ce 1.3779 | Loss_mse 0.1154\n",
      "Epoch 118/200 | Acc 0.5283 | Loss_ce 1.3779 | Loss_mse 0.1155\n",
      "Epoch 119/200 | Acc 0.5288 | Loss_ce 1.3770 | Loss_mse 0.1155\n",
      "Epoch 120/200 | Acc 0.5293 | Loss_ce 1.3770 | Loss_mse 0.1156\n",
      "Epoch 121/200 | Acc 0.5290 | Loss_ce 1.3770 | Loss_mse 0.1157\n",
      "Epoch 122/200 | Acc 0.5295 | Loss_ce 1.3760 | Loss_mse 0.1157\n",
      "Epoch 123/200 | Acc 0.5295 | Loss_ce 1.3760 | Loss_mse 0.1158\n",
      "Epoch 124/200 | Acc 0.5295 | Loss_ce 1.3760 | Loss_mse 0.1158\n",
      "Epoch 125/200 | Acc 0.5295 | Loss_ce 1.3750 | Loss_mse 0.1159\n",
      "Epoch 126/200 | Acc 0.5302 | Loss_ce 1.3750 | Loss_mse 0.1160\n",
      "Epoch 127/200 | Acc 0.5300 | Loss_ce 1.3750 | Loss_mse 0.1160\n",
      "Epoch 128/200 | Acc 0.5300 | Loss_ce 1.3750 | Loss_mse 0.1161\n",
      "Epoch 129/200 | Acc 0.5300 | Loss_ce 1.3740 | Loss_mse 0.1161\n",
      "Epoch 130/200 | Acc 0.5300 | Loss_ce 1.3740 | Loss_mse 0.1161\n",
      "Epoch 131/200 | Acc 0.5298 | Loss_ce 1.3740 | Loss_mse 0.1162\n",
      "Epoch 132/200 | Acc 0.5302 | Loss_ce 1.3740 | Loss_mse 0.1163\n",
      "Epoch 133/200 | Acc 0.5298 | Loss_ce 1.3740 | Loss_mse 0.1163\n",
      "Epoch 134/200 | Acc 0.5300 | Loss_ce 1.3730 | Loss_mse 0.1163\n",
      "Epoch 135/200 | Acc 0.5302 | Loss_ce 1.3730 | Loss_mse 0.1164\n",
      "Epoch 136/200 | Acc 0.5302 | Loss_ce 1.3730 | Loss_mse 0.1165\n",
      "Epoch 137/200 | Acc 0.5305 | Loss_ce 1.3730 | Loss_mse 0.1165\n",
      "Epoch 138/200 | Acc 0.5302 | Loss_ce 1.3730 | Loss_mse 0.1165\n",
      "Epoch 139/200 | Acc 0.5305 | Loss_ce 1.3730 | Loss_mse 0.1165\n",
      "Epoch 140/200 | Acc 0.5310 | Loss_ce 1.3730 | Loss_mse 0.1166\n",
      "Epoch 141/200 | Acc 0.5307 | Loss_ce 1.3721 | Loss_mse 0.1166\n",
      "Epoch 142/200 | Acc 0.5307 | Loss_ce 1.3721 | Loss_mse 0.1166\n",
      "Epoch 143/200 | Acc 0.5307 | Loss_ce 1.3721 | Loss_mse 0.1167\n",
      "Epoch 144/200 | Acc 0.5310 | Loss_ce 1.3721 | Loss_mse 0.1167\n",
      "Epoch 145/200 | Acc 0.5312 | Loss_ce 1.3721 | Loss_mse 0.1167\n",
      "Epoch 146/200 | Acc 0.5307 | Loss_ce 1.3721 | Loss_mse 0.1168\n",
      "Epoch 147/200 | Acc 0.5310 | Loss_ce 1.3721 | Loss_mse 0.1168\n",
      "Epoch 148/200 | Acc 0.5305 | Loss_ce 1.3721 | Loss_mse 0.1168\n",
      "Epoch 149/200 | Acc 0.5305 | Loss_ce 1.3721 | Loss_mse 0.1168\n",
      "Epoch 150/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1168\n",
      "Epoch 151/200 | Acc 0.5302 | Loss_ce 1.3711 | Loss_mse 0.1168\n",
      "Epoch 152/200 | Acc 0.5302 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 153/200 | Acc 0.5302 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 154/200 | Acc 0.5302 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 155/200 | Acc 0.5302 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 156/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 157/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 158/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 159/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 160/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1169\n",
      "Epoch 161/200 | Acc 0.5307 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 162/200 | Acc 0.5305 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 163/200 | Acc 0.5307 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 164/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 165/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 166/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 167/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 168/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 169/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 170/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 171/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 172/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1170\n",
      "Epoch 173/200 | Acc 0.5312 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 174/200 | Acc 0.5312 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 175/200 | Acc 0.5312 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 176/200 | Acc 0.5312 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 177/200 | Acc 0.5312 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 178/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 179/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 180/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 181/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 182/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 183/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 184/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 185/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 186/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 187/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 188/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 189/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 190/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 191/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 192/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 193/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 194/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 195/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 196/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 197/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 198/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 199/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n",
      "Epoch 200/200 | Acc 0.5310 | Loss_ce 1.3711 | Loss_mse 0.1171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisProjViT()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = proj.vit_proj.device  # projector already moved to GPU/CPU\n",
    "mse = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# copies needed for the L2 regularizer\n",
    "vit_proj_copy = proj.vit_proj.detach().clone()\n",
    "\n",
    "optimizer = torch.optim.Adam(proj.parameters(), lr=lr_v, eps=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_epoch)\n",
    "\n",
    "print(\"== Projector training ==\")\n",
    "proj.train()\n",
    "cnt = 0\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    loss_ce_hist, loss_mse_hist = [], []\n",
    "\n",
    "    if (cnt + 1) % aug_views == 0:\n",
    "        cnt = 0\n",
    "    else:\n",
    "        cnt += 1\n",
    "\n",
    "    train_x_before = train_x_before_list[cnt]\n",
    "    target = train_labels\n",
    "    feat_batch_size = cfg.get(\"feat_batch_size\", 0)\n",
    "\n",
    "    if feat_batch_size and feat_batch_size > 0:\n",
    "        N = train_x_before.size(0)\n",
    "        num_chunks = math.ceil(N / feat_batch_size)\n",
    "        lambda_scaled = lambda_v / float(max(num_chunks, 1))\n",
    "\n",
    "        for i0 in range(0, N, feat_batch_size):\n",
    "            i1 = min(i0 + feat_batch_size, N)\n",
    "            batch_x = train_x_before[i0:i1].to(device, non_blocking=True)\n",
    "            batch_y = target[i0:i1].to(device, non_blocking=True)\n",
    "\n",
    "            image_features = proj(batch_x)\n",
    "            image_features = F.normalize(image_features, dim=-1)\n",
    "            logits = 100.0 * image_features @ text_weights\n",
    "\n",
    "            initial_params = vit_proj_copy.view(-1)\n",
    "            fine_tuned_params = proj.vit_proj.view(-1)\n",
    "\n",
    "            mse_loss = mse(initial_params, fine_tuned_params)\n",
    "            loss_ce = F.cross_entropy(logits, batch_y)\n",
    "            loss = loss_ce + lambda_scaled * mse_loss\n",
    "\n",
    "            acc = cls_acc(logits, batch_y)\n",
    "            correct += acc / 100.0 * len(logits)\n",
    "            total += len(logits)\n",
    "            loss_ce_hist.append(loss_ce.item())\n",
    "            loss_mse_hist.append(mse_loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    else:\n",
    "        batch_x = train_x_before.to(device, non_blocking=True)\n",
    "        batch_y = target.to(device, non_blocking=True)\n",
    "\n",
    "        image_features = proj(batch_x)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        logits = 100.0 * image_features @ text_weights\n",
    "\n",
    "        initial_params = vit_proj_copy.view(-1)\n",
    "        fine_tuned_params = proj.vit_proj.view(-1)\n",
    "\n",
    "        mse_loss = mse(initial_params, fine_tuned_params)\n",
    "        loss_ce = F.cross_entropy(logits, batch_y)\n",
    "        loss = loss_ce + lambda_v * mse_loss\n",
    "\n",
    "        acc = cls_acc(logits, batch_y)\n",
    "        correct += acc / 100.0 * len(logits)\n",
    "        total += len(logits)\n",
    "        loss_ce_hist.append(loss_ce.item())\n",
    "        loss_mse_hist.append(mse_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{train_epoch} | Acc {correct/total:.4f} \"\n",
    "          f\"| Loss_ce {sum(loss_ce_hist)/len(loss_ce_hist):.4f} \"\n",
    "          f\"| Loss_mse {sum(loss_mse_hist)/len(loss_mse_hist):.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "proj.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9065ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hshi/anaconda3/envs/habcls/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hshi/Documents/researchproject/aihab/repo/aihab-clip\n",
      "SUBSAMPLE_CLASSES: all\n",
      "aug_views: 1\n",
      "backbone: ViT-B/16\n",
      "batch_size: 16\n",
      "data:\n",
      "  batch_size: 16\n",
      "  dataset_paths: ['/home/hshi/Documents/researchproject/aihab/repo/aihab-clip/data/CS_Xplots_2019_2023_train']\n",
      "  index_file_names: ['/home/hshi/Documents/researchproject/aihab/repo/aihab-clip/data/CS_Xplots_2019_2023_train/CS_Xplots_2019_23_NEW02OCT24.csv']\n",
      "  num_workers: 0\n",
      "  preprocessing:\n",
      "    augmentations:\n",
      "      bottom_crop: False\n",
      "      crop: ratio\n",
      "      flip: False\n",
      "      random_crop: True\n",
      "      rotation: True\n",
      "    resize: 439\n",
      "  shuffle: True\n",
      "dataset: cs\n",
      "feat_batch_size: 0\n",
      "lambda_funct_1_N: True\n",
      "lambda_funct_1_N2: False\n",
      "lambda_v: 0.1\n",
      "lr_v: 1e-05\n",
      "method: ProLIP\n",
      "output_dir: ./results\n",
      "projector:\n",
      "  checkpoint: None\n",
      "  enabled: False\n",
      "  eval_only: False\n",
      "  output_dir: ./results\n",
      "  require_cached_features: True\n",
      "resolution: 224\n",
      "root_path: ./\n",
      "save_features: False\n",
      "search_lr: False\n",
      "seed: 1\n",
      "shots: 0\n",
      "shuffle: True\n",
      "train_epoch: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from /home/hshi/Documents/researchproject/aihab/repo/aihab-clip/data/CS_Xplots_2019_2023_train: 100%|██████████| 4233/4233 [01:23<00:00, 50.71file/s]\n",
      "Loading images from /home/hshi/Documents/researchproject/aihab/repo/aihab-clip/data/CS_Xplots_2019_2023_test: 100%|██████████| 1398/1398 [00:24<00:00, 56.11file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader batches: 263, Test loader batches: 88\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test loader\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import load_cfg_from_cfg_file, merge_cfg_from_list\n",
    "from main import build_loaders\n",
    "\n",
    "base_cfg_path = PROJECT_ROOT / \"configs\" / \"base.yaml\"\n",
    "ds_cfg_path = PROJECT_ROOT / \"configs\" / \"cs.yaml\"\n",
    "\n",
    "data_cfg = load_cfg_from_cfg_file(str(base_cfg_path))\n",
    "data_cfg.update(load_cfg_from_cfg_file(str(ds_cfg_path)))\n",
    "\n",
    "data_cfg = deepcopy(data_cfg)\n",
    "\n",
    "\n",
    "def _resolve_paths(cfg, root):\n",
    "    data_cfg = cfg['data']\n",
    "    data_cfg['dataset_paths'] = [str((root / p).resolve()) for p in data_cfg['dataset_paths']]\n",
    "    data_cfg['index_file_names'] = [str((root / 'data/CS_Xplots_2019_2023_train'/ p).resolve()) for p in data_cfg['index_file_names']]\n",
    "    return cfg\n",
    "print(PROJECT_ROOT)\n",
    "data_cfg = _resolve_paths(data_cfg, PROJECT_ROOT)\n",
    "print(data_cfg)\n",
    "dl_tr, dl_te, train_tf, test_tf, info = build_loaders(data_cfg)\n",
    "print(f\"Train loader batches: {len(dl_tr)}, Test loader batches: {len(dl_te)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bde15b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 52.86%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "# 1) Extract pre-projection features/labels from the test loader\n",
    "test_features_before, test_labels = compute_image_features(clip_model, dl_te, to_cpu=True)\n",
    "device = proj.vit_proj.device\n",
    "test_features_before = test_features_before.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# 2) Apply the trained projector and evaluate\n",
    "with torch.no_grad():\n",
    "    test_features = proj(test_features_before)\n",
    "    test_features = F.normalize(test_features, dim=-1)\n",
    "    logits = 100.0 * test_features @ text_weights\n",
    "    preds = logits.argmax(dim=1)\n",
    "    acc = (preds == test_labels).float().mean().item() * 100.0\n",
    "\n",
    "print(f\"Test accuracy: {acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habcls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
